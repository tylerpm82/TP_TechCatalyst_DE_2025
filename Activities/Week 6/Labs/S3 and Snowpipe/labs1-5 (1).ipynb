{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50f91ec",
   "metadata": {},
   "source": [
    "| **Method**         | **Purpose**                                                                 | **Best Use Cases**                                                                 | **Pros**                                                                 | **Cons**                                                                 |\n",
    "|--------------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| `upload_file`      | Upload a file from local storage to an S3 bucket.                           | Large file uploads, simple file transfers.                                         | Handles multipart uploads automatically; easy to use.                   | Requires file path; not suitable for in-memory data.                    |\n",
    "| `upload_fileobj`   | Upload a file-like object to an S3 bucket.                                  | Uploading in-memory files (e.g., BytesIO), streaming data.                         | Flexible with file-like objects; supports streaming.                    | Requires manual multipart handling for large files.                    |\n",
    "| `put_object`       | Upload raw bytes or string data to an S3 bucket.                            | Small files, metadata updates, quick object creation.                              | Direct control over object content and metadata.                        | No automatic multipart support; less suitable for large files.         |\n",
    "| `download_file`    | Download a file from an S3 bucket to local storage.                         | Large file downloads, simple file retrieval.                                       | Handles multipart downloads; easy to use.                               | Requires file path; not suitable for in-memory processing.             |\n",
    "| `download_fileobj` | Download an S3 object to a file-like object.                                | In-memory processing, streaming downloads.                                         | Flexible with file-like objects; supports streaming.                    | Requires manual handling for large files.                              |\n",
    "| `get_object`       | Retrieve an object from S3, including its metadata and content.             | Reading object content directly, accessing metadata, partial downloads.            | Full access to object metadata and content; supports range requests.    | Requires manual handling for large files; response needs parsing.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e40801",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "### 1. Upload Methods\n",
    "\n",
    "**Q: What are the key differences between `upload_file`, `upload_fileobj`, and `put_object`?**  \n",
    "- `upload_file`: Uploads from local storage; supports automatic multipart uploads.  \n",
    "- `upload_fileobj`: Uploads from file-like objects (e.g., `BytesIO`); good for in-memory data.  \n",
    "- `put_object`: Uploads raw bytes or strings; best for small files and metadata updates.\n",
    "\n",
    "**Q: When would you choose to use `put_object` over `upload_file` or `upload_fileobj`?**  \n",
    "- When uploading small files or string data.  \n",
    "- When you need direct control over object content and metadata.  \n",
    "- When you donâ€™t need multipart upload support.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Download Methods\n",
    "**Q: How does `download_file` differ from `download_fileobj` and `get_object`?**  \n",
    "- `download_file`: Saves object to local file path; supports multipart downloads.  \n",
    "- `download_fileobj`: Streams object to a file-like object; ideal for in-memory processing.  \n",
    "- `get_object`: Returns object and metadata in a response dictionary; manual content handling required.\n",
    "\n",
    "**Q: In what scenarios would `get_object` be more beneficial than `download_file`?**  \n",
    "- When you need access to object metadata.  \n",
    "- When performing partial downloads using range headers.  \n",
    "- When integrating with other AWS services or needing fine-grained control.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Efficiency and Performance\n",
    "\n",
    "**Q: How do multipart uploads and downloads enhance the performance of file transfer operations?**  \n",
    "- They split large files into smaller parts and transfer them in parallel.  \n",
    "- Improve throughput and reduce timeouts.  \n",
    "- Allow retrying failed parts independently.\n",
    "\n",
    "**Q: What are the limitations of using `put_object` and `get_object` for large files?**  \n",
    "- `put_object`: No automatic multipart support; inefficient for large files.  \n",
    "- `get_object`: Requires manual handling of large content streams; memory-intensive.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Practical Applications\n",
    "\n",
    "**Q: Consider a scenario where you need to upload a large video file to S3. Which method would you use and why?**  \n",
    "- Use `upload_file` because it supports automatic multipart uploads and is optimized for large files.\n",
    "\n",
    "**Q: If you need to process data in memory before saving it locally, which download method would be most suitable?**  \n",
    "- Use `download_fileobj` because it allows streaming directly into a file-like object for in-memory manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25a0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.1 environment at: /workspaces/TechCatalyst_DE_2025/tdev\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 304ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install boto3 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f255b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # this assumes .env is inside the same folder you are working from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68379517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.1 environment at: /workspaces/TechCatalyst_DE_2025/tdev\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 69ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a704dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capstone-techcatalyst-conformed\n",
      "capstone-techcatalyst-raw\n",
      "capstone-techcatalyst-transformed\n",
      "techcatalyst-public\n",
      "techcatalyst-raw\n",
      "techcatalyst-transformed\n"
     ]
    }
   ],
   "source": [
    "# using Boto3 S3 client api\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# list bucket names\n",
    "buckets = s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']:\n",
    "    if 'techcatalyst' in bucket['Name']:\n",
    "        print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fb780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE/test.csv\n",
      "BLAKE/test4.csv\n",
      "BLAKE/test4.parquet/f02d2bad59de48efa69b0618bd89b99d.snappy.parquet\n",
      "BLAKE/test_export.parquet\n",
      "BLAKE/test_snowflake.csv\n",
      "BLAKE/test_snowflake2.csv\n",
      "BLAKE/test_snowflake3.csv\n",
      "BLAKE/upload_file_method_GOOG.csv\n",
      "BLAKE/upload_file_method_GOOG.parquet/25991cddfcc94a4fb93f5864d77a1fc7.snappy.parquet\n",
      "BLAKE/upload_fileobj_method.txt\n",
      "BLAKE_wr/9288e6c1eed4476d98eade7875cbf9c0.snappy.parquet\n",
      "BLAKE_wr/uploads/wr_newfile.csv\n",
      "Ben/Million_Songs/\n",
      "Ben/bingchilling.txt\n",
      "Ben/gooooog.csv\n",
      "Ben/parquetGoogleStock/a0eeb97dbd854a5c9cc6e89512faee73.snappy.parquet\n",
      "Ben/test.csv\n",
      "Ben/test2.csv\n",
      "Ben/uploads/Google_upload_test\n",
      "EMMA/emna_goog.csv\n",
      "EMMA/emna_goog.txt\n",
      "EMMA/test_export.parquet\n",
      "MELISSA/test.csv\n",
      "MELISSA/uploads/GOOG.csv\n",
      "MELISSA/uploads/fileobj.txt\n",
      "MELISSA/uploads/new_file.csv\n",
      "MillionSongSubset/\n",
      "MillionSongSubset/log-data/2018-11-01-events.json\n",
      "MillionSongSubset/log-data/2018-11-02-events.json\n",
      "MillionSongSubset/log-data/2018-11-03-events.json\n",
      "MillionSongSubset/log-data/2018-11-04-events.json\n",
      "MillionSongSubset/log-data/2018-11-05-events.json\n",
      "MillionSongSubset/log-data/2018-11-06-events.json\n",
      "MillionSongSubset/log-data/2018-11-07-events.json\n",
      "MillionSongSubset/log-data/2018-11-08-events.json\n",
      "MillionSongSubset/log-data/2018-11-09-events.json\n",
      "MillionSongSubset/log-data/2018-11-10-events.json\n",
      "MillionSongSubset/log-data/2018-11-11-events.json\n",
      "MillionSongSubset/log-data/2018-11-12-events.json\n",
      "MillionSongSubset/log-data/2018-11-13-events.json\n",
      "MillionSongSubset/log-data/2018-11-14-events.json\n",
      "MillionSongSubset/log-data/2018-11-15-events.json\n",
      "MillionSongSubset/log-data/2018-11-16-events.json\n",
      "MillionSongSubset/log-data/2018-11-17-events.json\n",
      "MillionSongSubset/log-data/2018-11-18-events.json\n",
      "MillionSongSubset/log-data/2018-11-19-events.json\n",
      "MillionSongSubset/log-data/2018-11-20-events.json\n",
      "MillionSongSubset/log-data/2018-11-21-events.json\n",
      "MillionSongSubset/log-data/2018-11-22-events.json\n",
      "MillionSongSubset/log-data/2018-11-23-events.json\n",
      "MillionSongSubset/log-data/2018-11-24-events.json\n",
      "MillionSongSubset/log-data/2018-11-25-events.json\n",
      "MillionSongSubset/log-data/2018-11-26-events.json\n",
      "MillionSongSubset/log-data/2018-11-27-events.json\n",
      "MillionSongSubset/log-data/2018-11-28-events.json\n",
      "MillionSongSubset/log-data/2018-11-29-events.json\n",
      "MillionSongSubset/log-data/2018-11-30-events.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAAW128F429D538.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAABD128F429CF47.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAADZ128F9348C2E.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAEF128F4273421.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAFD128F92F423A.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAMO128F1481E7F.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAMQ128F1460CD3.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAPK128E0786D96.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAARJ128F9320760.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAVG12903CFA543.json\n",
      "MillionSongSubset/song_data/A/A/A/TRAAAVO128F93133D4.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABCL128F4286650.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABDL12903CAABBA.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABJL12903CDCF1A.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABJV128F1460C49.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABLR128F423B7E3.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABNV128F425CEE1.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABRB128F9306DD5.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABVM128F92CA9DC.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABXG128F9318EBD.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABYN12903CFD305.json\n",
      "MillionSongSubset/song_data/A/A/B/TRAABYW128F4244559.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACCG128F92E8A55.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACER128F4290F96.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACFV128F935E50B.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACHN128F1489601.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACIW12903CC0F6D.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACLV128F427E123.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACNS128F14A2DF5.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACOW128F933E35F.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACPE128F421C1B9.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACQT128F9331780.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACSL128F93462F4.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACTB12903CAAF15.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACVS128E078BE39.json\n",
      "MillionSongSubset/song_data/A/A/C/TRAACZK128F4243829.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABACN128F425B784.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAFJ128F42AF24E.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAFP128F931E9A1.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAIO128F42938F9.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABATO128F42627E9.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAVQ12903CBF7E0.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAWW128F4250A31.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXL128F424FC50.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXR128F426515F.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAXV128F92F6AE3.json\n",
      "MillionSongSubset/song_data/A/B/A/TRABAZH128F930419A.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBAM128F429D223.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBBV128F42967D7.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBJE12903CDB442.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBKX128F4285205.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBLU128F93349CF.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBNP128F932546F.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBOP128F931B50D.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBOR128F4286200.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBTA128F933D304.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBVJ128F92F7EAA.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBXU128F92FEF48.json\n",
      "MillionSongSubset/song_data/A/B/B/TRABBZN12903CD9297.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCAJ12903CDFCC2.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCEC128F426456E.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCEI128F424C983.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCFL128F149BB0D.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCIX128F4265903.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCKL128F423A778.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCPZ128F4275C32.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCRU128F423F449.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCTK128F934B224.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCUQ128E0783E2B.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCXB128F4286BD3.json\n",
      "MillionSongSubset/song_data/A/B/C/TRABCYE128F934CE1D.json\n",
      "YOURNAME/test_export.parquet\n",
      "aamnah/e76ba6ddee764b198e4f34edde1cfbe4.snappy.parquet\n",
      "aamnah/uploads/GOOG_NEW.csv\n",
      "accidents/\n",
      "accidents/accidents_2017_to_2023_english.csv\n",
      "alexia/d306b442135f4ad082cf7edb2480f502.snappy.parquet\n",
      "alexia/uploads/new_stocks.csv\n",
      "camrenn.rivera/02a8203f7ed142a38f8c417b15188abc.snappy.parquet\n",
      "camrenn.rivera/30166a8ceeb64007a46abfbcdea8bd11.snappy.parquet\n",
      "camrenn.rivera/Io_test_file.txt\n",
      "camrenn.rivera/google_stock_downloaded.csv\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000000_000000.snappy.parquet\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000001_000000.snappy.parquet\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000002_000000.snappy.parquet\n",
      "emma/3a1b7394faa84991a5e7700c963af73f_000003_000000.snappy.parquet\n",
      "emma/uploads/wr_emma_stock\n",
      "fabiola/4ce04d7cf0854cd9a18baec4f5baa980.snappy.parquet\n",
      "fabiola/uploads/wr_GOOG.csv\n",
      "jaden/6e08eee074434aa7abda59ab22bc3ea7.snappy.parquet\n",
      "jaden/test.csv\n",
      "jaden/uploads/new_file.csv\n",
      "michael/49d5f54d6c4e46b8b4e01f07f7861c35.snappy.parquet\n",
      "michael/uploads/new_file_stocks.csv\n",
      "miraj/b4ec7d55de664e65992d4ae03886f3ff.snappy.parquet\n",
      "miraj/test.csv\n",
      "miraj_jara/uploads/google_stock_data\n",
      "shaswat/b4de0711c2d64b9c801bbdd02239e043.snappy.parquet\n",
      "shaswat/test.csv\n",
      "shaswat/uploads/test.csv\n",
      "stage/\n",
      "stage/yellow_tripdata.csv\n",
      "stage/yellow_tripdata.json\n",
      "stage/yellow_tripdata.parquet\n",
      "stocks/\n",
      "stocks/GOOG.csv\n",
      "suchitha/19027023cb9a4746b2e915ff13905e66.snappy.parquet\n",
      "suchitha/uploads/sales.csv\n",
      "tatwan/51addc9167684aa281c96fd5ae2d5be2.snappy.parquet\n",
      "tatwan/GOOG.csv\n",
      "tatwan/GOOG_NEW.csv\n",
      "tatwan/buffer.txt\n",
      "taxi_data/\n",
      "taxi_data/yellow_tripdata_2024-01.parquet\n",
      "taxi_data/yellow_tripdata_2024-02.parquet\n",
      "taxi_data/yellow_tripdata_2024-03.parquet\n",
      "taxi_data/yellow_tripdata_2024-04.parquet\n",
      "tyler/273176db78374c948307fa0c28b7d6b9.snappy.parquet\n",
      "tyler/test.csv\n",
      "tyler/test2.csv\n",
      "tyler/test3.csv\n",
      "tyler/test99.csv\n",
      "tyler/testidk.csv\n",
      "tyler/uploads/goog_stock_file.csv\n",
      "yellow_tripdata_2024-01.parquet\n",
      "yellow_tripdata_2024-02.parquet\n",
      "yellow_tripdata_2024-03.parquet\n",
      "yellow_tripdata_2024-04.parquet\n"
     ]
    }
   ],
   "source": [
    "# list objects in a specific bucket \"techcatalyst-raw\" \n",
    "bucket_name = 'techcatalyst-raw'\n",
    "objects = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "for obj in objects.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b28794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAKE/test.csv\n",
      "BLAKE/test4.csv\n",
      "BLAKE/test_snowflake.csv\n",
      "BLAKE/test_snowflake2.csv\n",
      "BLAKE/test_snowflake3.csv\n",
      "BLAKE/upload_file_method_GOOG.csv\n",
      "BLAKE_wr/uploads/wr_newfile.csv\n",
      "Ben/gooooog.csv\n",
      "Ben/test.csv\n",
      "Ben/test2.csv\n",
      "EMMA/emna_goog.csv\n",
      "MELISSA/test.csv\n",
      "MELISSA/uploads/GOOG.csv\n",
      "MELISSA/uploads/new_file.csv\n",
      "aamnah/uploads/GOOG_NEW.csv\n",
      "accidents/accidents_2017_to_2023_english.csv\n",
      "alexia/uploads/new_stocks.csv\n",
      "camrenn.rivera/google_stock_downloaded.csv\n",
      "fabiola/uploads/wr_GOOG.csv\n",
      "jaden/test.csv\n",
      "jaden/uploads/new_file.csv\n",
      "michael/uploads/new_file_stocks.csv\n",
      "miraj/test.csv\n",
      "shaswat/test.csv\n",
      "shaswat/uploads/test.csv\n",
      "stage/yellow_tripdata.csv\n",
      "stocks/GOOG.csv\n",
      "suchitha/uploads/sales.csv\n",
      "tatwan/GOOG.csv\n",
      "tatwan/GOOG_NEW.csv\n",
      "tyler/test.csv\n",
      "tyler/test2.csv\n",
      "tyler/test3.csv\n",
      "tyler/test99.csv\n",
      "tyler/testidk.csv\n",
      "tyler/uploads/goog_stock_file.csv\n"
     ]
    }
   ],
   "source": [
    "# list objects that are CSV in a specific bucket \"techcatalyst-raw\" \n",
    "for obj in objects.get('Contents', []):\n",
    "    if '.csv' in obj.get('Key'):\n",
    "        print(obj.get('Key'))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4c3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'techcatalyst-raw'\n",
    "\n",
    "s3_client.download_file(Bucket=bucket_name,  # from which bucket\n",
    "                        Key='stocks/GOOG.csv', # what is the object name, this include the directory/key.csv\n",
    "                        Filename='GOOG') # Filename is what you want to call it once it is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9eab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "io_temp = io.BytesIO()\n",
    "temp = s3_client.download_fileobj(Bucket=bucket_name, \n",
    "                           \t\t\tKey='stocks/GOOG.csv',\n",
    "                            \t\tFileobj=io_temp) # pass th io.BytesIO object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7046aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Date,Open,High,Low,Close,Volume\\r\\n1/2/2025 16:00:00,191.49,193.2,188.71,190.63,17545162\\r\\n1/3/2025 16:'\n"
     ]
    }
   ],
   "source": [
    "# show buffer content\n",
    "print(io_temp.getvalue()[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e14bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the content of the BytesIO object to a file \n",
    "with open('google_stock_downloaded.csv', 'wb') as f:\n",
    "    f.write(io_temp.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410a2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading a local file using upload_file\n",
    "s3_client.upload_file(Filename='google_stock_downloaded.csv', # local file name\n",
    "                      Bucket=bucket_name, # the bucket target\n",
    "                      Key='tyler/tyler_file.csv') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e6876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# uploading a local file using upload_file\n",
    "s3_client.upload_file(Filename='test.csv', # local file name\n",
    "                      Bucket=bucket_name, # the bucket target\n",
    "                      Key='tyler/last_test.csv') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff5dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_file = io.BytesIO(b\"Steven Universe and Adventure Time over Regular Show always\")\n",
    "s3_client.upload_fileobj(Fileobj=in_memory_file,\n",
    "                          Bucket=bucket_name, \n",
    "                          Key='tyler/tyler_another_file.txt') # destination name, make sure it include YOURNAME/ANY_FILE_NAME.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7419e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyler/3a63de0a979f4b22956d913074a48c24.snappy.parquet\n",
      "tyler/tyler_another_file.txt\n",
      "tyler/tyler_file.csv\n",
      "tyler/uploads/goog_stock_file.csv\n"
     ]
    }
   ],
   "source": [
    "# list objects in a specific bucket \"techcatalyst-raw\" with prefix \"tatwan\"\n",
    "# List objects with the specified prefix\n",
    "objects = s3_client.list_objects_v2(Bucket='techcatalyst-raw', Prefix='tyler')\n",
    "objects.keys()\n",
    "\n",
    "\n",
    "for obj in objects.get('Contents', []):\n",
    "    print(obj['Key'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37f4221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.1 environment at: /workspaces/TechCatalyst_DE_2025/tdev\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57e9985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load your credentials\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e8a9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import awswrangler\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "270b2904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date    Open    High     Low   Close    Volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if you can read from a private bucket\n",
    "\n",
    "df = wr.s3.read_csv('s3://techcatalyst-raw/stocks/GOOG.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e9dac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    140 non-null    object \n",
      " 1   Open    140 non-null    float64\n",
      " 2   High    140 non-null    float64\n",
      " 3   Low     140 non-null    float64\n",
      " 4   Close   140 non-null    float64\n",
      " 5   Volume  140 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# the returned object is actuall a pandas DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22553666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Database       Description\n",
      "0             aamnah_db                  \n",
      "1           aamnah_taxi                  \n",
      "2             alexia_db                  \n",
      "3           alexia_logs                  \n",
      "4           alexia_song                  \n",
      "5      awswrangler_test                  \n",
      "6                ben_db                  \n",
      "7              ben_song                  \n",
      "8              ben_taxi                  \n",
      "9            blake_taxi                  \n",
      "10          blake_wr_db                  \n",
      "11              default  default database\n",
      "12              emma_db                  \n",
      "13            emma_taxi                  \n",
      "14           fabiola_db                  \n",
      "15         fabiola_taxi                  \n",
      "16           jaden_taxi                  \n",
      "17        jadenastle_db                  \n",
      "18           melissa_db                  \n",
      "19         melissa_logs                  \n",
      "20        melissa_songs                  \n",
      "21         melissa_taxi                  \n",
      "22           michael_db                  \n",
      "23             miraj_db                  \n",
      "24           miraj_taxi                  \n",
      "25                my_db                  \n",
      "26           shaswat_db                  \n",
      "27         shaswat_logs                  \n",
      "28         shaswat_song                  \n",
      "29         shaswat_taxi                  \n",
      "30          suchitha_db                  \n",
      "31        suchitha_taxi                  \n",
      "32            tatwan_db                  \n",
      "33  tatwan_inclass_demo                  \n",
      "34          tatwan_taxi                  \n",
      "35             tyler_db                  \n",
      "36           tyler_taxi                  \n"
     ]
    }
   ],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b04fb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'tyler'\n",
    "database_name = f\"{name}_db\"\n",
    "wr.catalog.create_database(database_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d2cbfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Database       Description\n",
      "0             aamnah_db                  \n",
      "1           aamnah_taxi                  \n",
      "2             alexia_db                  \n",
      "3           alexia_logs                  \n",
      "4           alexia_song                  \n",
      "5      awswrangler_test                  \n",
      "6                ben_db                  \n",
      "7              ben_song                  \n",
      "8              ben_taxi                  \n",
      "9            blake_taxi                  \n",
      "10          blake_wr_db                  \n",
      "11              default  default database\n",
      "12              emma_db                  \n",
      "13            emma_taxi                  \n",
      "14           fabiola_db                  \n",
      "15         fabiola_taxi                  \n",
      "16           jaden_taxi                  \n",
      "17        jadenastle_db                  \n",
      "18           melissa_db                  \n",
      "19         melissa_logs                  \n",
      "20        melissa_songs                  \n",
      "21         melissa_taxi                  \n",
      "22           michael_db                  \n",
      "23             miraj_db                  \n",
      "24           miraj_taxi                  \n",
      "25                my_db                  \n",
      "26           shaswat_db                  \n",
      "27         shaswat_logs                  \n",
      "28         shaswat_song                  \n",
      "29         shaswat_taxi                  \n",
      "30          suchitha_db                  \n",
      "31        suchitha_taxi                  \n",
      "32            tatwan_db                  \n",
      "33  tatwan_inclass_demo                  \n",
      "34          tatwan_taxi                  \n",
      "35             tyler_db                  \n",
      "36           tyler_taxi                  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import awswrangler as wr\n",
    "\n",
    "# List all Glue databases\n",
    "databases = wr.catalog.databases()\n",
    "\n",
    "print(databases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37973721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Database, Table, Description, TableType, Columns, Partitions]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database='tyler_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7580f0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/tyler/441a4e0b45564c29953d1c54d4c5157c.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df, # the DataFrame you just created \n",
    "    path=f\"s3://techcatalyst-raw/tyler/\", # write to the techcatalyst-raw bucket under your folder name (or it would create a new folder if it does not exist)\n",
    "    dataset=True, \n",
    "    database='tyler_db', # the name of the database you just created in AWS Glue \n",
    "    table= 'tyler_stock', #YOUR CODE, # pick a table name for example YOURNAME_STOCK\n",
    "    mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5ef312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aamnah_db</td>\n",
       "      <td>aamnah_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexia_db</td>\n",
       "      <td>alexia_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ben_db</td>\n",
       "      <td>ben_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blake_wr_db</td>\n",
       "      <td>blake_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emma_db</td>\n",
       "      <td>emma_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fabiola_db</td>\n",
       "      <td>fabiola_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jadenastle_db</td>\n",
       "      <td>jaden_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>melissa_db</td>\n",
       "      <td>melissa_stocks</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>michael_db</td>\n",
       "      <td>michael_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>miraj_db</td>\n",
       "      <td>miraj_google_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>shaswat_db</td>\n",
       "      <td>shaswat_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>suchitha_db</td>\n",
       "      <td>suchitha_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td>tatwan_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tatwan_inclass_demo</td>\n",
       "      <td>tatwan_goog_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tyler_db</td>\n",
       "      <td>tyler_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Database               Table              Description  \\\n",
       "0             aamnah_db        aamnah_stock                            \n",
       "1             alexia_db        alexia_stock                            \n",
       "2                ben_db           ben_stock  This is my stock table.   \n",
       "3           blake_wr_db         blake_stock  This is my stock table.   \n",
       "4               emma_db          emma_stock  This is my stock table.   \n",
       "5            fabiola_db       fabiola_stock  This is my stock table.   \n",
       "6         jadenastle_db         jaden_stock  This is my stock table.   \n",
       "7            melissa_db      melissa_stocks  This is my stock table.   \n",
       "8            michael_db       michael_stock  This is my stock table.   \n",
       "9              miraj_db  miraj_google_stock  This is my stock table.   \n",
       "10           shaswat_db       shaswat_stock  This is my stock table.   \n",
       "11          suchitha_db      suchitha_stock  This is my stock table.   \n",
       "12            tatwan_db        tatwan_stock  This is my stock table.   \n",
       "13  tatwan_inclass_demo   tatwan_goog_stock                            \n",
       "14             tyler_db         tyler_stock                            \n",
       "\n",
       "         TableType                               Columns Partitions  \n",
       "0   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "1   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "2   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "3   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "4   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "5   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "6   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "7   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "8   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "9   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "10  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "11  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "12  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "13  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "14  EXTERNAL_TABLE  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(name_contains=\"stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8be8859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date    open    high     low   close    volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wr.s3.read_parquet_table(database='tyler_db', \n",
    "                              table='tyler_stock')\n",
    "\n",
    "# Display the DataFrame's first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0289d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 'string',\n",
       " 'open': 'double',\n",
       " 'high': 'double',\n",
       " 'low': 'double',\n",
       " 'close': 'double',\n",
       " 'volume': 'bigint'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.get_table_types(database='tyler_db', \n",
    "                           table='tyler_stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15c83389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'tyler_stock',\n",
       " 'DatabaseName': 'tyler_db',\n",
       " 'CreateTime': datetime.datetime(2025, 8, 4, 20, 13, 33, tzinfo=tzlocal()),\n",
       " 'UpdateTime': datetime.datetime(2025, 8, 5, 12, 9, 50, tzinfo=tzlocal()),\n",
       " 'Retention': 0,\n",
       " 'StorageDescriptor': {'Columns': [{'Name': 'date', 'Type': 'string'},\n",
       "   {'Name': 'open', 'Type': 'double'},\n",
       "   {'Name': 'high', 'Type': 'double'},\n",
       "   {'Name': 'low', 'Type': 'double'},\n",
       "   {'Name': 'close', 'Type': 'double'},\n",
       "   {'Name': 'volume', 'Type': 'bigint'}],\n",
       "  'Location': 's3://techcatalyst-raw/tyler/',\n",
       "  'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "  'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "  'Compressed': True,\n",
       "  'NumberOfBuckets': -1,\n",
       "  'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "   'Parameters': {'serialization.format': '1'}},\n",
       "  'BucketColumns': [],\n",
       "  'SortColumns': [],\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'compressionType': 'snappy',\n",
       "   'classification': 'parquet',\n",
       "   'typeOfData': 'file'},\n",
       "  'StoredAsSubDirectories': False},\n",
       " 'PartitionKeys': [],\n",
       " 'TableType': 'EXTERNAL_TABLE',\n",
       " 'Parameters': {'compressionType': 'snappy',\n",
       "  'classification': 'parquet',\n",
       "  'projection.enabled': 'false',\n",
       "  'typeOfData': 'file'},\n",
       " 'CreatedBy': 'arn:aws:iam::535146832369:user/Tyler.Powell',\n",
       " 'IsRegisteredWithLakeFormation': False,\n",
       " 'CatalogId': '535146832369',\n",
       " 'VersionId': '2',\n",
       " 'IsMultiDialectView': False}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_details = wr.catalog.get_tables(database='tyler_db')\n",
    "\n",
    "next(table_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2afc1327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/tyler/273176db78374c948307fa0c28b7d6b9.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example adding additional metadata information \n",
    "\n",
    "desc = \"This is my stock table.\"\n",
    "param = {\"source\": \"Google\", \"class\": \"stock\"}\n",
    "comments = {\n",
    "    \"Date\": \"Trading Date\",\n",
    "    \"Open\": \"Opening Price\",\n",
    "    \"Close\": \"Closing Price\"\n",
    "}\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path='s3://techcatalyst-raw/tyler/', # CHANGE THIS TO USE YOUR NAME for example s3://TECHCATALYST/TATWAN\n",
    "    dataset=True,\n",
    "    database='tyler_db',\n",
    "    table='tyler_stock',\n",
    "    mode='overwrite',\n",
    "    glue_table_settings=wr.typing.GlueTableSettings(description=desc,  # here we are passing some metadata\n",
    "                                                    parameters=param, \n",
    "                                                    columns_comments=comments),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9f56948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aamnah_db</td>\n",
       "      <td>aamnah_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexia_db</td>\n",
       "      <td>alexia_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ben_db</td>\n",
       "      <td>ben_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blake_wr_db</td>\n",
       "      <td>blake_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emma_db</td>\n",
       "      <td>emma_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fabiola_db</td>\n",
       "      <td>fabiola_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jadenastle_db</td>\n",
       "      <td>jaden_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>melissa_db</td>\n",
       "      <td>melissa_stocks</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>michael_db</td>\n",
       "      <td>michael_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>miraj_db</td>\n",
       "      <td>miraj_google_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>shaswat_db</td>\n",
       "      <td>shaswat_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>suchitha_db</td>\n",
       "      <td>suchitha_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td>tatwan_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tatwan_inclass_demo</td>\n",
       "      <td>tatwan_goog_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tyler_db</td>\n",
       "      <td>tyler_stock</td>\n",
       "      <td>This is my stock table.</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Database               Table              Description  \\\n",
       "0             aamnah_db        aamnah_stock                            \n",
       "1             alexia_db        alexia_stock                            \n",
       "2                ben_db           ben_stock  This is my stock table.   \n",
       "3           blake_wr_db         blake_stock  This is my stock table.   \n",
       "4               emma_db          emma_stock  This is my stock table.   \n",
       "5            fabiola_db       fabiola_stock  This is my stock table.   \n",
       "6         jadenastle_db         jaden_stock  This is my stock table.   \n",
       "7            melissa_db      melissa_stocks  This is my stock table.   \n",
       "8            michael_db       michael_stock  This is my stock table.   \n",
       "9              miraj_db  miraj_google_stock  This is my stock table.   \n",
       "10           shaswat_db       shaswat_stock  This is my stock table.   \n",
       "11          suchitha_db      suchitha_stock  This is my stock table.   \n",
       "12            tatwan_db        tatwan_stock  This is my stock table.   \n",
       "13  tatwan_inclass_demo   tatwan_goog_stock                            \n",
       "14             tyler_db         tyler_stock  This is my stock table.   \n",
       "\n",
       "         TableType                               Columns Partitions  \n",
       "0   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "1   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "2   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "3   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "4   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "5   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "6   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "7   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "8   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "9   EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "10  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "11  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "12  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "13  EXTERNAL_TABLE  date, open, high, low, close, volume             \n",
       "14  EXTERNAL_TABLE  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(name_contains=\"stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e71b05b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/tyler/273176db78374c948307fa0c28b7d6b9.snappy.parquet']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects('s3://techcatalyst-raw/tyler/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30e437ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.download(path='s3://techcatalyst-raw/stocks/GOOG.csv', \n",
    "               local_file='./goog_stock_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a33b6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'tyler'\n",
    "file_name = 'goog_stock_file.csv'\n",
    "wr.s3.upload(local_file='goog_stock_file.csv',path= f's3://techcatalyst-raw/{your_name}/uploads/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5367107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/tyler/uploads/goog_stock_file.csv']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects(f's3://techcatalyst-raw/{your_name}/uploads/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9efb9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"tyler_taxi\"\n",
    "\n",
    "table_name = \"tyler_tripdata\"\n",
    "\n",
    "s3_path_directory = \"s3://techcatalyst-raw/taxi_data/\"\n",
    "\n",
    "s3_path_file = \"s3://techcatalyst-raw/taxi_data/yellow_tripdata_2024-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf99f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read schema from Parquet file.\n"
     ]
    }
   ],
   "source": [
    "# uncomment below if you ran into issues to clean things up and rerun the cell\n",
    "wr.catalog.delete_table_if_exists(database=db_name, table=table_name) \n",
    "\n",
    "# Create the new Glue database first based on the db_name you created\n",
    "\n",
    "wr.catalog.create_database(name=db_name, exist_ok=True)\n",
    "\n",
    "# This function can extract the schema from our file and returns a tuple: (schema, partitions). We only need the schema. \n",
    "columns_types, partitions_types = wr.s3.read_parquet_metadata(path=s3_path_file)\n",
    "print(\"Successfully read schema from Parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "105b9f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'tyler_tripdata' created successfully in database 'tyler_taxi'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wr.catalog.create_parquet_table(\n",
    "    database=db_name, # pass the database name\n",
    "    table=table_name, # pass the table name\n",
    "    path=s3_path_directory, # use the directoy here \n",
    "    columns_types=columns_types,  # Pass the schema here\n",
    "    partitions_types=partitions_types\n",
    ")\n",
    "print(f\"Table '{table_name}' created successfully in database '{db_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c41be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Results:\n",
      "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2024-03-31 20:20:38   2024-03-31 20:45:00              1.0   \n",
      "1         2  2024-03-31 20:29:09   2024-03-31 20:45:20              2.0   \n",
      "2         2  2024-03-31 20:19:22   2024-04-01 00:00:00              1.0   \n",
      "3         2  2024-03-31 20:11:01   2024-03-31 20:29:34              1.0   \n",
      "4         2  2024-03-31 20:04:57   2024-03-31 20:11:51              4.0   \n",
      "\n",
      "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
      "0           9.10         1.0                  N            13           236   \n",
      "1           2.77         1.0                  N           230           113   \n",
      "2          10.01         1.0                  N            70            79   \n",
      "3           5.04         1.0                  N           138           202   \n",
      "4           0.82         1.0                  N           230            68   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1         38.0    3.5      0.5         4.0           0.0   \n",
      "1             1         17.0    1.0      0.5         4.4           0.0   \n",
      "2             1         40.1    6.0      0.5         0.0           0.0   \n",
      "3             1         24.0    6.0      0.5         6.3           0.0   \n",
      "4             2          7.9    0.0      0.5         0.0           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         47.00                   2.5         0.00  \n",
      "1                    1.0         26.40                   2.5         0.00  \n",
      "2                    1.0         51.85                   2.5         1.75  \n",
      "3                    1.0         39.55                   0.0         1.75  \n",
      "4                    1.0         11.90                   2.5         0.00  \n"
     ]
    }
   ],
   "source": [
    "query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
    "\n",
    "df = wr.athena.read_sql_query(query, database=db_name)\n",
    "\n",
    "print(\"\\nQuery Results:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef999c8",
   "metadata": {},
   "source": [
    "This solution enables automated data pipelines that convert incoming CSV files to Parquet format using AWS Lambda and awswrangler, catalog them with AWS Glue, and ingest them into Snowflake via Snowpipe. It's ideal for analytics, scalable ETL workflows, and efficient data lake-to-warehouse integration. I think it's cool that we can fully automate mundane processes like having to change the type of file that is being imported every time we bring one in. Specifically for my job on the data ingestion team this will be helpful if we need to migrate mass amounts of data that we have to configure for other processes later down the road."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
